Overview
Built an end-to-end data lakehouse on Azure using Databricks, ADF, and Delta Lake, with a Formula1 racing dataset. Implemented ETL pipelines with Bronze, Silver, and Gold layers, and optimized performance with partitioning and Delta features.

Tools & Technologies
Azure Databricks (PySpark)

Azure Data Lake Storage Gen2 (ADLS)

Azure Data Factory (ADF)

Delta Lake, Azure Key Vault

Features
Ingested structured/semi-structured data from ADLS using mount points

Transformed data using PySpark notebooks

Implemented partitioning, deduplication, and schema evolution

Used OPTIMIZE and VACUUM to manage file sizes and performance

Loaded curated data into Gold layer for reporting
